# ðŸŒ Web Crawler CLI Tool

A command-line based **web crawler** built using **Node.js** that recursively extracts and analyzes internal links from a given website.  
It provides a detailed report showing how many times each internal page was found â€” helping understand site structure and link connectivity.

---

## ðŸš€ Features
- ðŸ•¸ï¸ **Recursive Crawling:** Traverses all internal links within the same domain.  
- ðŸ§© **HTML Parsing:** Extracts links efficiently using the `JSDOM` library.  
- ðŸ”— **URL Normalization:** Converts relative and absolute links into a consistent format.  
- âš™ï¸ **Asynchronous Fetching:** Uses `async/await` for non-blocking network requests.  
- ðŸ§  **Error Handling:** Handles broken links, non-HTML responses, and invalid URLs.  
- ðŸ“Š **Reporting Module:** Displays link frequency and page stats sorted by count.  
- ðŸ§± **Modular Design:** Cleanly divided into crawling logic, reporting, and CLI execution files.  

---

## ðŸ› ï¸ Tech Stack
- **Language:** JavaScript (ES6)
- **Runtime:** Node.js
- **Libraries:** JSDOM, Fetch API

---

## ðŸ“‚ Project Structure
```plaintext
web-crawler-cli/
â”‚
â”œâ”€â”€ crawl.js        â†’ Core crawler logic (fetches and parses internal links)
â”œâ”€â”€ report.js       â†’ Generates and prints crawl reports
â””â”€â”€ main.js         â†’ Entry point for CLI execution
```
## âš™ï¸ How It Works

1. Takes a **base URL** as a command-line argument.  
2. Recursively **fetches internal pages** from that URL.  
3. Parses **HTML content** using `JSDOM` to extract all links.  
4. Generates a **report** showing how many times each page was found.

## ðŸ§‘â€ðŸ’» How to Run
1. **Clone the Repository**
2. ```bash
   git clone https://github.com/namitaa15/web_crawlerhttp.git  
   cd web_crawlerhttp
   ```

3. **Install Dependencies**
   npm install  

4. **Run the Application**  
   You can run the crawler in two ways:  
   ðŸŸ¢ Using npm script â†’  npm start https://example.com  
   ðŸŸ£ Using Node directly â†’  node main.js https://example.com  
 

>  Replace `https://example.com` with any valid URL to analyze its internal link structure.
